{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question-Question MaLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-eTMk_CbtN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, model_from_json\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda, Softmax, Dense, Concatenate\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta, Adam\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8MrLIMqijt9",
        "colab_type": "code",
        "outputId": "6e2336da-49ba-4c5e-9a25-d87467d0a823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "TRAIN_CSV = '/content/drive/My Drive/Sem 3-2/NLA/Project/train.csv'\n",
        "TEST_CSV = '/content/drive/My Drive/Sem 3-2/NLA/Project/test.csv'\n",
        "MODEL_SAVING_DIR = '/content/drive/My Drive/Sem 3-2/NLA/Project/Models/'\n",
        "\n",
        "# Make changes here for embedding and model name\n",
        "MODEL_NAME = 'lstm_glove_manhattan'\n",
        "EMBEDDING_TYPE = 'glove' # w2v, glove\n",
        "\n",
        "if EMBEDDING_TYPE == 'w2v':\n",
        "  EMBEDDING_FILE = '/content/drive/My Drive/Sem 3-2/NLA/Project/GoogleNews-vectors-negative300.bin.gz'\n",
        "elif EMBEDDING_TYPE == 'glove':\n",
        "  EMBEDDING_FILE = '/content/drive/My Drive/Sem 3-2/NLA/Project/glove/glove.6B.100d.txt'"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6FctGEE3Uz4",
        "colab_type": "code",
        "outputId": "341d9dba-db5d-49d7-9871-3abfcb7b0d76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vk6eIIUlwNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load training and test set\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Prepare embedding\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "\n",
        "if EMBEDDING_TYPE == 'w2v':\n",
        "  word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "elif EMBEDDING_TYPE == 'glove':\n",
        "  embeddings_index = dict()\n",
        "  glove_vocab = []\n",
        "  f = open(EMBEDDING_FILE)  # glove.6B.100d.txt\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    glove_vocab.append(word)\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "questions_cols = ['question1', 'question2']\n",
        "\n",
        "# Iterate over the questions only of both training and test datasets\n",
        "for index, row in train_df.iterrows():\n",
        "\n",
        "    for question in questions_cols:\n",
        "\n",
        "        q2n = []  # q2n -> question numbers representation\n",
        "        for word in text_to_word_list(row[question]):\n",
        "\n",
        "            # Stop word removal\n",
        "            if EMBEDDING_TYPE == 'w2v':\n",
        "              if word in stops and word not in word2vec.vocab:\n",
        "                  continue\n",
        "                  \n",
        "            elif EMBEDDING_TYPE == 'glove':\n",
        "              if word in stops and word not in glove_vocab:\n",
        "                continue\n",
        "\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = len(inverse_vocabulary)\n",
        "                q2n.append(len(inverse_vocabulary))\n",
        "                inverse_vocabulary.append(word)\n",
        "            else:\n",
        "                q2n.append(vocabulary[word])\n",
        "\n",
        "        # Replace questions as word to question as number representation\n",
        "        train_df.at[index, question] = q2n\n",
        "\n",
        "if EMBEDDING_TYPE == 'w2v':            \n",
        "  embedding_dim = 300\n",
        "  embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "  embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "  # Build the embedding matrix\n",
        "  for word, index in vocabulary.items():\n",
        "      if word in word2vec.vocab:\n",
        "          embeddings[index] = word2vec.word_vec(word)\n",
        "  del word2vec\n",
        "\n",
        "elif EMBEDDING_TYPE == 'glove':\n",
        "  embedding_dim = 100\n",
        "  vocabulary_size = len(vocabulary) + 1\n",
        "  embeddings = np.zeros((vocabulary_size, 100))\n",
        "  for word, index in vocabulary.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embeddings[index] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ZIE6zEl1Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
        "                     train_df.question2.map(lambda x: len(x)).max())\n",
        "\n",
        "# Split to train validation test\n",
        "test_size = 40000\n",
        "validation_size = 40000\n",
        "training_size = len(train_df) - validation_size - test_size\n",
        "\n",
        "X = train_df[questions_cols]\n",
        "Y = train_df['is_duplicate']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_size)\n",
        "\n",
        "# Split to dicts\n",
        "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
        "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
        "X_test = {'left': X_test.question1, 'right': X_test.question2}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n",
        "Y_test = Y_test.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train, X_validation, X_test], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4lwWwU305ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    prec = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2*((prec*rec)/(prec + rec + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgrR9Pi0l3rS",
        "colab_type": "code",
        "outputId": "2d882077-87cc-4229-fc61-d3f02ac4839a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "source": [
        "# Model variables\n",
        "n_hidden = 64\n",
        "gradient_clipping_norm = 1.25\n",
        "batch_size = 2048\n",
        "n_epoch = 20\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# Embedded version of the inputs\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "shared_lstm = LSTM(n_hidden)\n",
        "\n",
        "left_output = shared_lstm(encoded_left)\n",
        "right_output = shared_lstm(encoded_right)\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "# Distance computation varies in different models\n",
        "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# Adam optimizer, with learning rate\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy', recall, precision, f1])\n",
        "\n",
        "training_start_time = time()\n",
        "\n",
        "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
        "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 324290 samples, validate on 40000 samples\n",
            "Epoch 1/20\n",
            "324290/324290 [==============================] - 230s 710us/step - loss: 0.2132 - accuracy: 0.6984 - recall: 0.4004 - precision: 0.6421 - f1: 0.4842 - val_loss: 0.1780 - val_accuracy: 0.7326 - val_recall: 0.4912 - val_precision: 0.6937 - val_f1: 0.5750\n",
            "Epoch 2/20\n",
            "324290/324290 [==============================] - 228s 702us/step - loss: 0.1720 - accuracy: 0.7434 - recall: 0.5270 - precision: 0.7042 - f1: 0.6025 - val_loss: 0.1669 - val_accuracy: 0.7557 - val_recall: 0.5629 - val_precision: 0.7142 - val_f1: 0.6294\n",
            "Epoch 3/20\n",
            "324290/324290 [==============================] - 225s 694us/step - loss: 0.1640 - accuracy: 0.7595 - recall: 0.5703 - precision: 0.7208 - f1: 0.6364 - val_loss: 0.1614 - val_accuracy: 0.7665 - val_recall: 0.5915 - val_precision: 0.7247 - val_f1: 0.6512\n",
            "Epoch 4/20\n",
            "324290/324290 [==============================] - 225s 695us/step - loss: 0.1590 - accuracy: 0.7699 - recall: 0.5959 - precision: 0.7315 - f1: 0.6564 - val_loss: 0.1576 - val_accuracy: 0.7742 - val_recall: 0.6186 - val_precision: 0.7282 - val_f1: 0.6688\n",
            "Epoch 5/20\n",
            "324290/324290 [==============================] - 227s 700us/step - loss: 0.1553 - accuracy: 0.7770 - recall: 0.6139 - precision: 0.7388 - f1: 0.6703 - val_loss: 0.1542 - val_accuracy: 0.7788 - val_recall: 0.6216 - val_precision: 0.7372 - val_f1: 0.6744\n",
            "Epoch 6/20\n",
            "324290/324290 [==============================] - 225s 695us/step - loss: 0.1523 - accuracy: 0.7828 - recall: 0.6266 - precision: 0.7450 - f1: 0.6803 - val_loss: 0.1515 - val_accuracy: 0.7844 - val_recall: 0.6205 - val_precision: 0.7518 - val_f1: 0.6797\n",
            "Epoch 7/20\n",
            "324290/324290 [==============================] - 229s 707us/step - loss: 0.1500 - accuracy: 0.7874 - recall: 0.6380 - precision: 0.7499 - f1: 0.6888 - val_loss: 0.1497 - val_accuracy: 0.7885 - val_recall: 0.6379 - val_precision: 0.7514 - val_f1: 0.6899\n",
            "Epoch 8/20\n",
            "324290/324290 [==============================] - 228s 703us/step - loss: 0.1480 - accuracy: 0.7906 - recall: 0.6457 - precision: 0.7527 - f1: 0.6947 - val_loss: 0.1484 - val_accuracy: 0.7903 - val_recall: 0.6343 - val_precision: 0.7581 - val_f1: 0.6906\n",
            "Epoch 9/20\n",
            "324290/324290 [==============================] - 228s 704us/step - loss: 0.1463 - accuracy: 0.7940 - recall: 0.6537 - precision: 0.7562 - f1: 0.7008 - val_loss: 0.1470 - val_accuracy: 0.7937 - val_recall: 0.6610 - val_precision: 0.7503 - val_f1: 0.7027\n",
            "Epoch 10/20\n",
            "324290/324290 [==============================] - 224s 691us/step - loss: 0.1448 - accuracy: 0.7964 - recall: 0.6579 - precision: 0.7594 - f1: 0.7046 - val_loss: 0.1460 - val_accuracy: 0.7949 - val_recall: 0.6663 - val_precision: 0.7496 - val_f1: 0.7054\n",
            "Epoch 11/20\n",
            "324290/324290 [==============================] - 222s 685us/step - loss: 0.1436 - accuracy: 0.7986 - recall: 0.6631 - precision: 0.7616 - f1: 0.7083 - val_loss: 0.1452 - val_accuracy: 0.7994 - val_recall: 0.6871 - val_precision: 0.7487 - val_f1: 0.7164\n",
            "Epoch 12/20\n",
            "324290/324290 [==============================] - 222s 686us/step - loss: 0.1422 - accuracy: 0.8010 - recall: 0.6676 - precision: 0.7642 - f1: 0.7124 - val_loss: 0.1442 - val_accuracy: 0.8003 - val_recall: 0.6792 - val_precision: 0.7546 - val_f1: 0.7148\n",
            "Epoch 13/20\n",
            "324290/324290 [==============================] - 223s 689us/step - loss: 0.1410 - accuracy: 0.8028 - recall: 0.6718 - precision: 0.7662 - f1: 0.7155 - val_loss: 0.1430 - val_accuracy: 0.8008 - val_recall: 0.6712 - val_precision: 0.7608 - val_f1: 0.7131\n",
            "Epoch 14/20\n",
            "324290/324290 [==============================] - 225s 695us/step - loss: 0.1401 - accuracy: 0.8038 - recall: 0.6729 - precision: 0.7678 - f1: 0.7167 - val_loss: 0.1425 - val_accuracy: 0.7998 - val_recall: 0.6502 - val_precision: 0.7711 - val_f1: 0.7054\n",
            "Epoch 15/20\n",
            "324290/324290 [==============================] - 222s 686us/step - loss: 0.1390 - accuracy: 0.8059 - recall: 0.6766 - precision: 0.7702 - f1: 0.7201 - val_loss: 0.1418 - val_accuracy: 0.8009 - val_recall: 0.6471 - val_precision: 0.7759 - val_f1: 0.7055\n",
            "Epoch 16/20\n",
            "324290/324290 [==============================] - 223s 687us/step - loss: 0.1383 - accuracy: 0.8072 - recall: 0.6796 - precision: 0.7720 - f1: 0.7224 - val_loss: 0.1411 - val_accuracy: 0.8055 - val_recall: 0.6845 - val_precision: 0.7633 - val_f1: 0.7216\n",
            "Epoch 17/20\n",
            "324290/324290 [==============================] - 224s 689us/step - loss: 0.1374 - accuracy: 0.8086 - recall: 0.6817 - precision: 0.7734 - f1: 0.7243 - val_loss: 0.1406 - val_accuracy: 0.8029 - val_recall: 0.6495 - val_precision: 0.7787 - val_f1: 0.7081\n",
            "Epoch 18/20\n",
            "324290/324290 [==============================] - 224s 691us/step - loss: 0.1366 - accuracy: 0.8103 - recall: 0.6849 - precision: 0.7758 - f1: 0.7271 - val_loss: 0.1398 - val_accuracy: 0.8059 - val_recall: 0.6853 - val_precision: 0.7640 - val_f1: 0.7224\n",
            "Epoch 19/20\n",
            "324290/324290 [==============================] - 222s 684us/step - loss: 0.1359 - accuracy: 0.8115 - recall: 0.6874 - precision: 0.7771 - f1: 0.7292 - val_loss: 0.1394 - val_accuracy: 0.8073 - val_recall: 0.6938 - val_precision: 0.7622 - val_f1: 0.7262\n",
            "Epoch 20/20\n",
            "324290/324290 [==============================] - 222s 686us/step - loss: 0.1351 - accuracy: 0.8128 - recall: 0.6892 - precision: 0.7791 - f1: 0.7310 - val_loss: 0.1390 - val_accuracy: 0.8069 - val_recall: 0.6718 - val_precision: 0.7745 - val_f1: 0.7194\n",
            "Training time finished.\n",
            "20 epochs in 1:15:02.234231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46hMuYjL2rDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = malstm.to_json()\n",
        "with open(MODEL_SAVING_DIR + MODEL_NAME + \".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "malstm.save_weights(MODEL_SAVING_DIR + MODEL_NAME + \".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Ias4_YlwZy",
        "colab_type": "code",
        "outputId": "df43a9e3-57cb-4e0a-9c87-f476a5a06ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "loss, acc, rec, prec, f1 = malstm.evaluate([X_test['left'], X_test['right']], Y_test)\n",
        "print(\"Loss =\", loss, \"Accuracy =\", acc, \"Recall =\", rec, \"Precision =\", prec, \"F1 =\", f1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "pred = malstm.predict([X_test['left'], X_test['right']])\n",
        "cm = confusion_matrix(Y_test, pred.round())\n",
        "cm_img = sn.heatmap(cm, annot=True)\n",
        "\n",
        "figure = cm_img.get_figure()    \n",
        "figure.savefig(MODEL_SAVING_DIR + MODEL_NAME + '.png', dpi=400)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 214s 5ms/step\n",
            "Loss = 0.13918368144631385 Accuracy = 0.8045750260353088 Recall = 0.6704756617546082 Precision = 0.7667705416679382 F1 = 0.7055104970932007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShzPX6Nw3O8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load json and create model\n",
        "json_file = open(MODEL_SAVING_DIR + MODEL_NAME + \".json\", 'rb')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "malstm = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "malstm.load_weights(MODEL_SAVING_DIR + MODEL_NAME + \".h5\")\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy', recall, precision, f1])\n",
        "\n",
        "loss, acc, rec, prec, f1 = malstm.evaluate([X_test['left'], X_test['right']], Y_test)\n",
        "print(\"Loss =\", loss, \"Accuracy =\", acc, \"Recall =\", rec, \"Precision =\", prec, \"F1 =\", f1)\n",
        "\n",
        "pred = malstm.predict([X_test['left'], X_test['right']])\n",
        "cm = confusion_matrix(Y_test, pred.round())\n",
        "sn.heatmap(cm, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYNA0BEQl6Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot accuracy\n",
        "plt.plot(malstm_trained.history['accuracy'])\n",
        "plt.plot(malstm_trained.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(malstm_trained.history['loss'])\n",
        "plt.plot(malstm_trained.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAr5PXuR9Mj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}