{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question-Question BiLSTM Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-eTMk_CbtN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "from __future__ import division, print_function\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, model_from_json, Sequential\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda, Softmax, Dense, Concatenate, Dropout, Add, add, concatenate, GRU, Reshape, Flatten, dot, Bidirectional\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta, Adam\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ISx3h9yOZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8MrLIMqijt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_CSV = '/content/drive/My Drive/Sem 3-2/NLA/Project/train.csv'\n",
        "TEST_CSV = '/content/drive/My Drive/Sem 3-2/NLA/Project/test.csv'\n",
        "MODEL_SAVING_DIR = '/content/drive/My Drive/Sem 3-2/NLA/Project/Models/'\n",
        "\n",
        "# Make changes here for embedding and model name\n",
        "MODEL_NAME = 'bilstm_attn_w2v_softmax'\n",
        "EMBEDDING_TYPE = 'w2v' # w2v, glove\n",
        "\n",
        "if EMBEDDING_TYPE == 'w2v':\n",
        "  EMBEDDING_FILE = '/content/drive/My Drive/Sem 3-2/NLA/Project/GoogleNews-vectors-negative300.bin.gz'\n",
        "elif EMBEDDING_TYPE == 'glove':\n",
        "  EMBEDDING_FILE = '/content/drive/My Drive/Sem 3-2/NLA/Project/glove/glove.6B.100d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx0FbK8eyT74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vk6eIIUlwNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load training and test set\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Prepare embedding\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "\n",
        "if EMBEDDING_TYPE == 'w2v':\n",
        "  word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "elif EMBEDDING_TYPE == 'glove':\n",
        "  embeddings_index = dict()\n",
        "  glove_vocab = []\n",
        "  f = open(EMBEDDING_FILE)  # glove.6B.100d.txt\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    glove_vocab.append(word)\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "questions_cols = ['question1', 'question2']\n",
        "\n",
        "# Iterate over the questions only of both training and test datasets\n",
        "for index, row in train_df.iterrows():\n",
        "\n",
        "    for question in questions_cols:\n",
        "\n",
        "        q2n = []  # q2n -> question numbers representation\n",
        "        for word in text_to_word_list(row[question]):\n",
        "\n",
        "            # Stop word removal\n",
        "            if EMBEDDING_TYPE == 'w2v':\n",
        "              if word in stops and word not in word2vec.vocab:\n",
        "                  continue\n",
        "                  \n",
        "            elif EMBEDDING_TYPE == 'glove':\n",
        "              if word in stops and word not in glove_vocab:\n",
        "                continue\n",
        "\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = len(inverse_vocabulary)\n",
        "                q2n.append(len(inverse_vocabulary))\n",
        "                inverse_vocabulary.append(word)\n",
        "            else:\n",
        "                q2n.append(vocabulary[word])\n",
        "\n",
        "        # Replace questions as word to question as number representation\n",
        "        train_df.at[index, question] = q2n\n",
        "\n",
        "if EMBEDDING_TYPE == 'w2v':            \n",
        "  embedding_dim = 300\n",
        "  embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "  embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "  # Build the embedding matrix\n",
        "  for word, index in vocabulary.items():\n",
        "      if word in word2vec.vocab:\n",
        "          embeddings[index] = word2vec.word_vec(word)\n",
        "  del word2vec\n",
        "\n",
        "elif EMBEDDING_TYPE == 'glove':\n",
        "  embedding_dim = 100\n",
        "  vocabulary_size = len(vocabulary) + 1\n",
        "  embeddings = np.zeros((vocabulary_size, 100))\n",
        "  for word, index in vocabulary.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embeddings[index] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ZIE6zEl1Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
        "                     train_df.question2.map(lambda x: len(x)).max())\n",
        "\n",
        "# Split to train validation test\n",
        "test_size = 40000\n",
        "validation_size = 40000\n",
        "training_size = len(train_df) - validation_size - test_size\n",
        "\n",
        "X = train_df[questions_cols]\n",
        "Y = train_df['is_duplicate']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_size)\n",
        "\n",
        "# Split to dicts\n",
        "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
        "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
        "X_test = {'left': X_test.question1, 'right': X_test.question2}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n",
        "Y_test = Y_test.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train, X_validation, X_test], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4lwWwU305ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    prec = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2*((prec*rec)/(prec + rec + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyKELgOE4olR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model variables\n",
        "n_hidden = 64\n",
        "gradient_clipping_norm = 1.25\n",
        "batch_size = 1024\n",
        "n_epoch = 10\n",
        "\n",
        "def get_model(input_shape):\n",
        "  left_input = Input(shape=(input_shape,), dtype='int32')\n",
        "  right_input = Input(shape=(input_shape,), dtype='int32')\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=input_shape, trainable=False))\n",
        "  \n",
        "  model.add(Bidirectional(LSTM(n_hidden, return_sequences=True)))\n",
        "  model.add(Bidirectional(LSTM(n_hidden, return_sequences=True)))\n",
        "  model.add(Bidirectional(LSTM(n_hidden)))\n",
        "\n",
        "  left_output = model(left_input)\n",
        "  right_output = model(right_input)\n",
        "\n",
        "  # attention model\n",
        "  attn = Sequential()\n",
        "  attn.add(Flatten())\n",
        "  attn.add(Dense((input_shape * n_hidden)))\n",
        "  attn.add(Reshape((input_shape, n_hidden)))\n",
        "\n",
        "  dot_product = dot([left_output, right_output], axes=1, normalize=False)\n",
        "  attn_output = dot_product\n",
        "  \n",
        "  # classification\n",
        "  addition_layer = Lambda(lambda tensors:(tensors[0]+tensors[1]))([left_output, attn_output])\n",
        "  prediction = Dense(1, activation='sigmoid')(addition_layer)\n",
        "\n",
        "  siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
        "  return siamese_net\n",
        "\n",
        "lstm = get_model(max_seq_length)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', recall, precision, f1])\n",
        "\n",
        "# Start training\n",
        "training_start_time = time()\n",
        "\n",
        "lstm_trained = lstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
        "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46hMuYjL2rDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = lstm.to_json()\n",
        "with open(MODEL_SAVING_DIR + MODEL_NAME +\".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "lstm.save_weights(MODEL_SAVING_DIR + MODEL_NAME + \".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Ias4_YlwZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, acc, rec, prec, f1 = lstm.evaluate([X_test['left'], X_test['right']], Y_test)\n",
        "print(\"Loss =\", loss, \"Accuracy =\", acc, \"Recall =\", rec, \"Precision =\", prec, \"F1 =\", f1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "pred = lstm.predict([X_test['left'], X_test['right']])\n",
        "cm = confusion_matrix(Y_test, pred.round())\n",
        "cm_img = sn.heatmap(cm, annot=True)\n",
        "\n",
        "figure = cm_img.get_figure()    \n",
        "figure.savefig(MODEL_SAVING_DIR + MODEL_NAME + '.png', dpi=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShzPX6Nw3O8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load json and create model\n",
        "json_file = open(MODEL_SAVING_DIR + MODEL_NAME + \".json\", 'rb')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "lstm = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "lstm.load_weights(MODEL_SAVING_DIR + MODEL_NAME +\".h5\")\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', recall, precision, f1])\n",
        "\n",
        "loss, acc, rec, prec, f1 = lstm.evaluate([X_test['left'], X_test['right']], Y_test)\n",
        "print(\"Loss =\", loss, \"Accuracy =\", acc, \"Recall =\", rec, \"Precision =\", prec, \"F1 =\", f1)\n",
        "\n",
        "pred = lstm.predict([X_test['left'], X_test['right']])\n",
        "cm = confusion_matrix(Y_test, pred.round())\n",
        "sn.heatmap(cm, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alwJoZdhK_dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYNA0BEQl6Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot accuracy\n",
        "plt.plot(lstm_trained.history['accuracy'])\n",
        "plt.plot(lstm_trained.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(lstm_trained.history['loss'])\n",
        "plt.plot(lstm_trained.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAr5PXuR9Mj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}